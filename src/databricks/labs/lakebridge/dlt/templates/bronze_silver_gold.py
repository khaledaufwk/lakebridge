# Databricks notebook source
# MAGIC %md
# MAGIC # Bronze-Silver-Gold DLT Pipeline
# MAGIC 
# MAGIC This pipeline implements the medallion architecture:
# MAGIC - **Bronze**: Raw data ingestion
# MAGIC - **Silver**: Cleaned and validated data
# MAGIC - **Gold**: Business-level aggregates
# MAGIC 
# MAGIC Generated by Lakebridge Migration Tool

# COMMAND ----------

import dlt
from pyspark.sql import functions as F

# COMMAND ----------

# MAGIC %md
# MAGIC ## Bronze Layer - Raw Data Ingestion

# COMMAND ----------

@dlt.table(
    name="bronze_raw_data",
    comment="Raw data from source system",
    table_properties={
        "quality": "bronze",
        "pipelines.autoOptimize.managed": "true"
    }
)
def bronze_raw_data():
    """Ingest raw data from source."""
    return (
        spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "json")
        .option("cloudFiles.inferColumnTypes", "true")
        .load("/mnt/source/raw_data/")
    )

# COMMAND ----------

# MAGIC %md
# MAGIC ## Silver Layer - Cleaned Data

# COMMAND ----------

@dlt.expect_or_drop("valid_id", "id IS NOT NULL")
@dlt.expect("valid_timestamp", "created_at IS NOT NULL")
@dlt.table(
    name="silver_cleaned_data",
    comment="Cleaned and validated data",
    table_properties={
        "quality": "silver",
        "pipelines.autoOptimize.managed": "true"
    }
)
def silver_cleaned_data():
    """Clean and validate data from bronze layer."""
    return (
        dlt.read_stream("bronze_raw_data")
        .withColumn("processed_at", F.current_timestamp())
        .withColumn("id", F.col("id").cast("long"))
        .dropDuplicates(["id"])
    )

# COMMAND ----------

# MAGIC %md
# MAGIC ## Gold Layer - Business Aggregates

# COMMAND ----------

@dlt.table(
    name="gold_daily_summary",
    comment="Daily business summary",
    table_properties={
        "quality": "gold",
        "pipelines.autoOptimize.managed": "true"
    }
)
def gold_daily_summary():
    """Aggregate silver data into daily summaries."""
    return (
        dlt.read("silver_cleaned_data")
        .withColumn("date", F.to_date("created_at"))
        .groupBy("date")
        .agg(
            F.count("*").alias("total_records"),
            F.countDistinct("id").alias("unique_ids"),
        )
    )
